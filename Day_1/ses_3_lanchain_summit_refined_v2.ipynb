{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmPxtgLniVcT"
      },
      "source": [
        "# LangChain Crash Course\n",
        "\n",
        "**Structure of this notebook**:\n",
        "1. Setup & imports\n",
        "2. LangChain basics (chains, memory, tools, agents)\n",
        "3. A small game\n",
        "4. Upcoming session\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dc0401"
      },
      "source": [
        "## 1. Setup & Imports\n",
        "\n",
        "In this section we:\n",
        "\n",
        "- Install all the Python packages we will use later (LangChain core, tools, RAG helpers, widgets).\n",
        "- Pull in the **core classes** (prompts, chains, memory, tools, agents, document loaders, vector stores).\n",
        "- Configure the environment once, so the later sections can focus only on ideas and examples.\n",
        "\n",
        "Run these cells once when you open the notebook. After that, you can jump straight to the basics or the mini‚Äëprojects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICD1vxdxiUK_",
        "outputId": "9118cfb7-5756-4ae0-8b5e-0cd372804133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m106 packages\u001b[0m \u001b[2min 1.26s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m9 packages\u001b[0m \u001b[2min 1.27s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 60ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfaiss-cpu\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install langchain-community pydrive2 faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhG6NRVeA9zD",
        "outputId": "48ba2de2-95fe-4ad5-8917-ebd98d8c8d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[37m‚†ã\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ã\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mpypdf==6.4.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 49ms\u001b[0m\u001b[0m\n",
            "\u001b[37m‚†ã\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37m‚†ã\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2m\u001b[0m (1/1)                                                                        \r\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
            "‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mpypdf==6.4.1                                         \u001b[0m\r\u001b[2K‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [1/1] \u001b[2mpypdf==6.4.1                                         \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==6.4.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq31PYLpEjNW"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YGJnuTqFrTS"
      },
      "outputs": [],
      "source": [
        "# Install the necessary packages for the chain\n",
        "!uv pip install -q langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9X-5d8oJLUM"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH5ajpwmOMW2"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q duckduckgo-search wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWWhks5DSdF9",
        "outputId": "e6fb2949-211d-483f-d205-d0edd57218c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m17 packages\u001b[0m \u001b[2min 186ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 185ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mddgs\u001b[0m\u001b[2m==9.9.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfake-useragent\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msocksio\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -U ddgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWW5im54W7jD"
      },
      "outputs": [],
      "source": [
        "# 1. Install the Hub (for pulling the standard agent prompt)\n",
        "!uv pip install -q langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-ugemPSXRmo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWa4-4NLiWlg"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3431302b"
      },
      "outputs": [],
      "source": [
        "# Core imports for this notebook\n",
        "\n",
        "# Standard library\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Optional: Colab Drive (uncomment if using Colab)\n",
        "# from google.colab import drive\n",
        "\n",
        "# Display + widgets for simple UI\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# LangChain core primitives\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# LangChain Google Gemini\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LangChain tools & agents\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import create_agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b93d4e5"
      },
      "source": [
        "### Google API key & LLM setup\n",
        "\n",
        "Make sure you have your `GOOGLE_API_KEY` set securely (e.g., via Colab secrets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqYjIlRuJxx7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Get API key from Colab secrets manager\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2eef32e"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Shared LLM used in all LangChain examples\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemma-3-27b-it\", #\"gemini-2.5-flash-lite\",\n",
        "    temperature=0.7,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de94037a"
      },
      "source": [
        "# 2. LangChain: Let's get started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC2DLAadx_V5"
      },
      "source": [
        "## Pain points\n",
        "> 1. LLMs are stateless, and can only access current context (input). Context management is crucial.\n",
        "\n",
        "> 2. The code for intercommunication of Models, Memory, Tools, is a lot of struggle.\n",
        "\n",
        "## Solutions?\n",
        "\n",
        "\n",
        "> Let's make a smart context manager, that can help us with prompt templates, prompt templates.\n",
        "\n",
        "> Let's standardise the communication between any two components.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### What is **Grammar**?\n",
        "### Why do we need **Grammar** in language?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG9fFZWVEzh2"
      },
      "source": [
        "## Components\n",
        "\n",
        "<!-- ![alt text](https://drive.google.com/uc?export=view&id=1YDf49ft1iV81tg1nWfTfFcbDZda9OfTm) -->\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1HPMN42P2w6Grn38mb5Rxre7OEVzK47oE)\n",
        "<!--\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1HPMN42P2w6Grn38mb5Rxre7OEVzK47oE\" width=\"1500\"> -->\n",
        "\n",
        "\n",
        "1. Chains\n",
        "2. Memory\n",
        "3. Agents & Tools\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oleTv_I7Q8BW"
      },
      "source": [
        "## 2.1. Chains\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13btaEa3XJQbV_YwRJgK4xJ5loeKGiSEK)\n",
        "\n",
        "LangChain‚Äôs core abstraction is the **chain**:\n",
        "\n",
        "> input ‚Üí prompt ‚Üí LLM ‚Üí parser ‚Üí output\n",
        "\n",
        "In this section you will see:\n",
        "\n",
        "- `ChatPromptTemplate` - how we define parameterised prompts like `\"Tell me a joke about {topic}\"`.\n",
        "- `StrOutputParser` - a simple parser that turns the model's response into a plain Python string.\n",
        "- **LCEL composition (`|`)** - how to connect prompt ‚Üí model ‚Üí parser into a single reusable object.\n",
        "- How to **compose chains**: one chain writes a short story, another reviews it, and we plug them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUs30GnSLUyQ",
        "outputId": "fc648072-26d8-4c52-f868-a72630728f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the software engineer move to Pune?\n",
            "\n",
            "...Because they heard the traffic was a great debugging challenge! \n",
            "\n",
            "(Pune is known for its notoriously challenging traffic!)\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Create the Prompt\n",
        "# It expects a dictionary like {\"topic\": \"...\"}\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
        "\n",
        "# 2. Create the Parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 3. Build the Chain using LCEL\n",
        "# The data flows from left to right\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# 4. Run the Chain\n",
        "response = chain.invoke({\"topic\": \"software engineers\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzXq3X7f_RTn"
      },
      "outputs": [],
      "source": [
        "# Chain 1: The Comedian\n",
        "prompt1 = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
        "chain1 = prompt1 | llm | StrOutputParser()\n",
        "\n",
        "# Chain 2: The Audience\n",
        "# Note: This prompt expects the input to be the joke text itself\n",
        "prompt2 = ChatPromptTemplate.from_template(\"Write a sarcastic reply on did you find this funny: {joke}\")\n",
        "chain2 = prompt2 | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFncjjG2L63U"
      },
      "outputs": [],
      "source": [
        "respons1 = chain1.invoke({\"topic\": \"teddy bear\"})\n",
        "respons2 = chain2.invoke({\"joke\": respons1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wl0rMmuaMBlR",
        "outputId": "22ab133c-90ee-480e-cb47-bb038e99564a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the teddy bear say no to dessert? \\n\\n... Because she was stuffed! \\n\\nüòÇ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "respons1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oKULBY2PMFCF",
        "outputId": "8c527b8e-fd58-41aa-9a43-27dfda3dc002"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh, *hilarious*. Truly groundbreaking comedy. I haven't laughed that hard since... well, since the last time someone told that joke. It's so original, so unexpected. My sides are *aching* from the sheer wit. üòÇ (Please send help, I think I'm developing a permanent eye-roll.) \\n\\nSeriously though, it's a classic. A *very* classic. You've really outdone yourself. üôÑ\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "respons2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvh43wdBMGRf",
        "outputId": "d1da1842-d2cc-42d8-b463-e007580d1339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh. My. God. Absolutely *riveting*. I haven't laughed this hard since... well, probably since the last time someone told me a teddy bear joke. Truly, a comedic masterpiece. I'm going to need a moment to recover from the sheer brilliance. üòÇ (Is that the level of sarcasm you were hoping for?)\n"
          ]
        }
      ],
      "source": [
        "# The 'RunnablePassthrough' or a simple dictionary map helps us bridge the two.\n",
        "# We map the output of chain1 to the input key \"story\" for chain2.\n",
        "overall_chain = {\"joke\": chain1} | chain2\n",
        "\n",
        "# Run it!\n",
        "print(overall_chain.invoke({\"topic\": \"teddy bear\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL6Da8h_RHWh"
      },
      "source": [
        "## 2.2 Memory\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1I2bPOe47vmLiUfyQEVaMXXNXrY0hBQH1)\n",
        "By default, chains are **stateless**: every call forgets previous turns.\n",
        "\n",
        "To build a real chat experience we need **memory**, i.e., a way to keep track of past messages and feed them back to the model. In this example we use:\n",
        "\n",
        "- `ChatMessageHistory` - an in-memory list of messages for each conversation.\n",
        "- `MessagesPlaceholder(\"history\")` - a slot in the prompt where we inject the past messages.\n",
        "- `RunnableWithMessageHistory` - a wrapper that automatically:\n",
        "  - reads the right history for a given `session_id`\n",
        "  - appends new user/assistant messages after each call.\n",
        "\n",
        "After running the example, the model correctly answers *‚ÄúWhat did I just tell you about my work?‚Äù* by reading from the stored history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NAikfjfvOLRS"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Define a chat prompt with a history placeholder\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"You are a friendly assistant that remembers details about the user.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 2. In-memory store for multiple sessions\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# 3. Wrap chain with RunnableWithMessageHistory\n",
        "chat_chain_with_memory = RunnableWithMessageHistory(\n",
        "    chat_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "session_id = \"demo-session\"\n",
        "\n",
        "for user_input in [\n",
        "    \"Hi, I'm Chinmay and I work with LangChain.\",\n",
        "    \"What did I just tell you about my work?\",\n",
        "]:\n",
        "    response = chat_chain.invoke(\n",
        "        {\"input\": user_input, \"history\": [\"\"]},\n",
        "        config={\"configurable\": {\"session_id\": session_id}},\n",
        "    )\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {response}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 4. Demo conversation\n",
        "session_id = \"demo-session\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEevRrkifZcs",
        "outputId": "a8f502b6-6221-4cad-d43f-a1fc962341ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py:2745: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi, I'm Chinmay and I work with LangChain.\n",
            "Assistant: Hi Chinmay! It's great to meet you. So you're Chinmay, and you work with LangChain - that's fantastic! I'll remember that. \n",
            "\n",
            "LangChain is really interesting stuff - are you building anything cool with it at the moment? I'm always eager to hear what people are doing with these kinds of tools.\n",
            "\n",
            "Just let me know if I can help you with anything at all. üòä\n",
            "--------------------------------------------------\n",
            "User: What did I just tell you about my work?\n",
            "Assistant: You just told me you're a software engineer working on a project involving a large language model! Specifically, you mentioned you're working on making it more reliable and less prone to \"hallucinations\" - making things up. \n",
            "\n",
            "It sounds like interesting work! üòä Is there anything I can help you with regarding that, or anything else?\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for user_input in [\n",
        "    \"Hi, I'm Chinmay and I work with LangChain.\",\n",
        "    \"What did I just tell you about my work?\",\n",
        "]:\n",
        "    response = chat_chain_with_memory.invoke(\n",
        "        {\"input\": user_input},\n",
        "        config={\"configurable\": {\"session_id\": session_id}},\n",
        "    )\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {response}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk9nqMa0fbTP",
        "outputId": "e749540f-d641-47c0-c9d4-e24c4ec7a641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi, I'm Chinmay and I work with LangChain.\n",
            "Assistant: Hi Chinmay! It's great to meet you. So you're Chinmay, and you work with LangChain - that's fantastic! I'll remember that. \n",
            "\n",
            "LangChain is really interesting stuff - are you building anything cool with it at the moment? I'm always eager to hear what people are doing with these kinds of tools.\n",
            "\n",
            "Just let me know if I can help you with anything at all. üòä\n",
            "--------------------------------------------------\n",
            "User: What did I just tell you about my work?\n",
            "Assistant: You just told me that you're Chinmay and you work with LangChain! I'm doing my best to remember things - glad to see I got that one right. üòä \n",
            "\n",
            "Is there anything specific about your work with LangChain you'd like to talk about?\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Agents and Tools\n",
        "![](https://drive.google.com/uc?export=view&id=1Ppm_f4xRHIjnTexrHoY0_98zCU8CIFXg)"
      ],
      "metadata": {
        "id": "5dmd2ZKKvWX5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viQWKTV4Rv2l"
      },
      "source": [
        "### 2.3.1 Adding Tools\n",
        "\n",
        "Tools wrap **external capabilities** and present them to the LLM as callable functions.\n",
        "\n",
        "Here we define two tools:\n",
        "\n",
        "- `DuckDuckGoSearchRun` - for general web search.\n",
        "- `WikipediaQueryRun` - for quick encyclopaedic lookups.\n",
        "\n",
        "We then:\n",
        "\n",
        "1. Put them into a `tools` list.\n",
        "2. Call `search.run(...)` directly to see what a tool returns.\n",
        "3. Reuse the same `tools` list later when we build agents and the guessing game.\n",
        "\n",
        "The key idea: tools are your bridge from *LLM thinking* to *real-world actions* (APIs, DB calls, code, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijWqROs7RzJi",
        "outputId": "24cb9140-7fc4-48e6-e353-3d45e58752ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Test: Get detailed information about the Alphabet Inc. stock (GOOGL) including Price , Charts, Technical Analysis, Historical data, Alphabet Reports and more. View live Alphabet Inc ( Google ) Class C chart to track its stock 's price action. Find market predictions, GOOG financials and market news.The current price of GOOG is 322.09 USD ‚Äî it has increased by 1.16% in the past 24 hours. Find the latest Alphabet Inc. (GOOG) stock quote, history, news and other vital information to help you with your stock trading and investing.Buy. Analyst Price Targets. 185.00 Low. 320.43 Average. 322.09 Current . The stock currently trades at a forward price -to-earnings ratio of 18 times based on analysts' estimates for 2025. Find stock quotes, interactive charts, historical information, company news and stock analysis on all public companies from Nasdaq.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# 1. Define the Search Tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# 2. Define the Wikipedia Tool\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "# 3. Create a \"Toolbelt\" (List of tools)\n",
        "tools = [search, wikipedia]\n",
        "\n",
        "# Test the tools individually to see what they return\n",
        "print(\"Search Test:\", search.run(\"Current stock price of Google\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFVhzm42S6GY"
      },
      "source": [
        "### 2.3.2 Add Agents\n",
        "\n",
        "Agents are **LLM‚Äëpowered controllers**.\n",
        "\n",
        "Given:\n",
        "\n",
        "- a model (`llm`), and\n",
        "- a list of tools (`tools`),\n",
        "\n",
        "an agent will decide **which tool to call, with what arguments, and when to stop**. Internally, the loop looks like:\n",
        "\n",
        "1. Think ‚Üí decide what to do.\n",
        "2. Act ‚Üí call a tool.\n",
        "3. Observe ‚Üí read the result.\n",
        "4. Repeat until it has an answer.\n",
        "\n",
        "In this section we use `create_agent` to hide that loop behind a single convenient interface, then let the agent answer a question using the search + Wikipedia tools you defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\", # Changed from gemma-3-27b-it to a model that supports function calling\n",
        "    temperature=0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "1PobX8f6y6xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTAe9MKdSZCN",
        "outputId": "1fc1265b-e717-4c3f-e1a1-99a4406ed47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "# 1. Create the Agent\n",
        "# This \"all-in-one\" function creates a compiled agent graph.\n",
        "# It handles the loop (Thought -> Action -> Observation) automatically.\n",
        "agent = create_agent(\n",
        "    model=llm_gemini,\n",
        "    tools=tools,\n",
        "    system_prompt=\"You are a helpful assistant. Use your tools to answer questions.\"\n",
        ")\n",
        "\n",
        "# 2. Run the Agent\n",
        "# Note: We pass 'messages' because this is a Chat Agent.\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"human\", \"Who is the current CEO of Google and how old is he?\")]\n",
        "})\n",
        "\n",
        "# 3. Print the Result\n",
        "# The response is a dictionary. The final answer is usually the last message.\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dpU0eRMes_j"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_project_info(project_code: str) -> str:\n",
        "    '''\n",
        "    Gets project info from the database.\n",
        "    '''\n",
        "    return f\"Info about project {project_code}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RqecM0llaeZ"
      },
      "outputs": [],
      "source": [
        "tools.append(get_project_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjKf4EI7lzSx"
      },
      "outputs": [],
      "source": [
        "# help(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 More on memory\n",
        "![](https://drive.google.com/uc?export=view&id=1Tbiq_2bAmbLaK5wl_00hnFbpC_4z1mPa)"
      ],
      "metadata": {
        "id": "FBHh80Al1GFT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg3YHWPD4C3Y"
      },
      "source": [
        "## 2.5 Langchain Ecosystem\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1M9Q3f1oGEWy09tlTLLi4QITJeg7rDCEj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QeQWgbdiTHE"
      },
      "source": [
        "## 3. Mini Project - Guess Who Agent with Tools (Notebook UI)\n",
        "\n",
        "In this game:\n",
        "\n",
        "- **You** secretly think of a famous real or fictional person.\n",
        "- The **agent** asks you questions (mostly yes/no) to figure out who it is.\n",
        "- The agent can optionally use **web search and Wikipedia tools** internally to check facts.\n",
        "\n",
        "We also add a tiny notebook UI using `ipywidgets` so you can play without writing `input()` loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJCRhDfOiTHc"
      },
      "source": [
        "### 3.1 Create the Guess Who agent (with web tools)\n",
        "\n",
        "We reuse the `search` and `wikipedia` tools defined earlier and give them to a new agent.\n",
        "The agent follows a simple protocol:\n",
        "\n",
        "- If it's asking a question: start the message with `QUESTION:`  \n",
        "- If it's making a guess: start the message with `GUESS:`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define a callback that sleeps after each LLM call\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "\n",
        "class SleepAfterLLMHandler(BaseCallbackHandler):\n",
        "    def __init__(self, delay: float = 2.0):\n",
        "        self.delay = delay\n",
        "\n",
        "    def on_llm_end(self, *args, **kwargs):\n",
        "        # This is triggered after EVERY LLM call inside the agent\n",
        "        time.sleep(self.delay)"
      ],
      "metadata": {
        "id": "ytI_mYj2s2UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91181fb9-5cef-4725-fcbd-7596b4674f59",
        "id": "DWitlTHkiTHd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guess Who tool-using agent is ready!\n"
          ]
        }
      ],
      "source": [
        "# --- Guess Who agent that can use web tools ---\n",
        "\n",
        "guess_who_tool_agent = create_agent(\n",
        "    model=llm_gemini,\n",
        "    tools=tools,  # reuses [search, wikipedia] from above\n",
        "    system_prompt=(\n",
        "        \"You are playing a game called 'Guess Who' which is a game like Akinator. \"\n",
        "        \"The user is secretly thinking of a well-known real or fictional person.\\n\\n\"\n",
        "        \"Your goal is to identify the person by asking a sequence of questions.\\n\"\n",
        "        \"Also try to ask smart questions as you have to keep the number of questions minimum.\\n\"\n",
        "        \"- Ask one concise question at a time.\\n\"\n",
        "        \"- Prefer YES/NO questions, but short open questions are okay if needed.\\n\"\n",
        "        \"- After you think you know the answer, make a guess.\\n\\n\"\n",
        "        \"You have access to web search and Wikipedia tools to look up facts about people \"\n",
        "        \"(for example, to check if a candidate matches the clues you have).\\n\\n\"\n",
        "        \"Output protocol:\\n\"\n",
        "        \"- If you are asking a question, start with 'QUESTION: '.\\n\"\n",
        "        \"- If you are making a guess, start with 'GUESS: '.\\n\"\n",
        "        \"Do not show tool call details to the user; use them internally.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Guess Who tool-using agent is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMc-1GViTHe"
      },
      "source": [
        "### 3.2 Simple notebook UI with `ipywidgets`\n",
        "\n",
        "Use the buttons + text box to:\n",
        "\n",
        "1. Click **Start New Game** - the agent will ask the first question.\n",
        "2. Type your answer in the text box and click **Send Answer** each turn.\n",
        "3. When the agent prints `GUESS: ...`, you can decide if it was correct and either keep playing or start a new game.\n",
        "\n",
        "> Tip: Think of a person *before* you click **Start New Game** üôÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0715cb78dc404cfeb8b31a6deac4d69e",
            "1925be229de44bd4bdb2a2b9f8e6238c",
            "17474e2fe7264daa955535094ff9140b",
            "11f3a856286a4e47b66b31fdebbeca55",
            "fe9e02bce892480db8e6aceed53a5a81",
            "dbf15071ff9c40ae859979232d0c1c4b",
            "c4676a4f23fb469894110f619219a177",
            "0fa45c5022ed4c24a11f9d665bbedb11",
            "b90b6487c76f4a78b9091219c40c66dd",
            "cda99f4c8496445d89cee8f5a6252fcb",
            "887c117f4b4f4bc0a4be78138e655bb0",
            "063b1ec1ca43464d9783264a40b2fdbb",
            "eed2522da4f44ea0979175150ffef2ef",
            "491da9dfb3ab48d995c35e7124577b97",
            "98bd46bc703b40f3884011ec96f7d289"
          ]
        },
        "outputId": "97a96b89-826b-4b8f-8189-911193637f1b",
        "id": "_Mc6U95UiTHe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Button(button_style='success', description='Start New Game', style=ButtonStyle()), HBox(childre‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0715cb78dc404cfeb8b31a6deac4d69e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Simple UI for playing Guess Who inside the notebook\n",
        "\n",
        "# Widgets\n",
        "start_button = widgets.Button(description=\"Start New Game\", button_style=\"success\")\n",
        "answer_box = widgets.Text(placeholder=\"Type your answer here (yes/no/short text)...\")\n",
        "send_button = widgets.Button(description=\"Send Answer\", button_style=\"primary\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Conversation state (messages list used by the agent)\n",
        "guess_messages = []\n",
        "\n",
        "def start_game(_):\n",
        "    global guess_messages\n",
        "    guess_messages = [(\"human\", \"START\")]\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"Think of a famous real or fictional person, then answer the questions.\")\n",
        "        print(\"Agent is thinking (20 seconds)...\")\n",
        "        time.sleep(20)  # 20-second delay before first request\n",
        "        response = guess_who_tool_agent.invoke({\"messages\": guess_messages})\n",
        "        guess_messages = response[\"messages\"]\n",
        "        print(\"Agent:\", guess_messages[-1].content)\n",
        "\n",
        "def send_answer(_):\n",
        "    global guess_messages\n",
        "    user_text = answer_box.value.strip()\n",
        "    if not user_text:\n",
        "        return\n",
        "    answer_box.value = \"\"\n",
        "\n",
        "    guess_messages.append((\"human\", user_text))\n",
        "\n",
        "    with output_area:\n",
        "        print(\"\\nYou:\", user_text)\n",
        "        print(\"‚è≥ Agent is thinking (20 seconds)...\")\n",
        "        # time.sleep(20)  # 20-second delay before each follow-up request\n",
        "        response = guess_who_tool_agent.invoke(\n",
        "            {\n",
        "                \"messages\": guess_messages\n",
        "            },\n",
        "            config={\n",
        "                \"callbacks\": [SleepAfterLLMHandler(delay=20)]  # 20-second pause after EACH LLM call\n",
        "            },\n",
        "        )\n",
        "        guess_messages = response[\"messages\"]\n",
        "        print(\"Agent:\", guess_messages[-1].content)\n",
        "\n",
        "start_button.on_click(start_game)\n",
        "send_button.on_click(send_answer)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    start_button,\n",
        "    widgets.HBox([answer_box, send_button]),\n",
        "    output_area,\n",
        "])\n",
        "\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6291cae2"
      },
      "source": [
        "## UPCOMING - RAG\n",
        "\n",
        "In this section we build a tiny **retrieval-augmented generation (RAG)** pipeline:\n",
        "\n",
        "1. **Load** documents from a folder with `DirectoryLoader` + `PyPDFLoader`.\n",
        "2. **Split** them into overlapping text chunks with `RecursiveCharacterTextSplitter`.\n",
        "3. **Embed & index** the chunks in a FAISS vector store using a BGE embedding model.\n",
        "4. **Ask questions**: a RAG chain retrieves relevant chunks and lets Gemini answer *based only on those documents*.\n",
        "\n",
        "This is the ‚Äúserious‚Äù mini-project, compared to the game: it shows how the same LangChain building blocks\n",
        "(prompts, chains, retrievers) can power a real-world document-QA workflow."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}